{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CustomEmbedding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTd2jZ1Mwod0",
        "colab_type": "text"
      },
      "source": [
        "# Text Classification. Hash Vectorizer and SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYns0wYYvHN5",
        "colab_type": "text"
      },
      "source": [
        "We get ~87% classification accuracy on the \"ag news\" dataset, with a Hashing Vectorizer and an SVM based on gradient descent. No special optimizations were made. The state of the art for this dataset is ~92.5%. See for example\n",
        "1. R. Johnson, T. Zhang, *Effective Use of Word Order for Text Categorization\n",
        "with Convolutional Neural Networks*\n",
        "2. X. Zhan, Y. LeCun, *Text Understanding from Scratch*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAUn9gxdsnYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip uninstall --yes tensorflow\n",
        "!pip install tensorflow==2.0.0\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EySG4K5zsqWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.util import ngrams \n",
        "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
        "logging.root.level = logging.INFO\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfz137CNst83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doc_cleaner(doc):\n",
        "    # doc = doc.decode('utf-8')\n",
        "    doc = doc.lower()        \n",
        "    doc = re.sub(r':?\\\\+', ' ', doc)  # remove double backslash\n",
        "    # doc = re.sub(r'[0-9]+', ' ', doc)  # remove numbers\n",
        "    # doc = re.sub(r'#[\\w-]+', ' ', doc)  # remove hashtag\n",
        "    # doc = re.sub(r'^[^-]* - ', ' ', doc)  # remove everything before the first hyphen\n",
        "    doc = re.sub(r'[^A-Z a-z]+', '', doc)\n",
        "    doc = re.sub(' +', ' ', doc)  # multiple spaces\n",
        "    return doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrAUaWYQswtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embedding(tf.keras.Model):\n",
        "    def __init__(self, kernel_size, batch_size, embedding_length):\n",
        "        super().__init__()\n",
        "        self.model = tf.keras.Sequential()\n",
        "        self.model.add(tf.keras.layers.Conv1D(filters=3, kernel_size=kernel_size, input_shape=(embedding_length, 1)))\n",
        "        self.model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "        # self.model.add(tf.keras.layers.Flatten())\n",
        "        self.model.add(tf.keras.layers.Dense(units=4, activation='softmax'))        \n",
        "        \n",
        "        # optimizer and loss funciton\n",
        "        self.optimizer = tf.keras.optimizers.Adam()\n",
        "        self.loss_fun = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "        self.model.compile(optimizer=self.optimizer, loss=self.loss_fun, metrics=['categorical_accuracy'])  \n",
        "\n",
        "    def fit_data(self, docvecs, target, batch_size, epochs):\n",
        "        \"\"\"  supports both raw data and generators \"\"\"\n",
        "        history = self.model.fit(docvecs,\n",
        "                                target,\n",
        "                                batch_size=batch_size,\n",
        "                                epochs=epochs)\n",
        "        print(\"training history: {}\".format(history))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzqaMftGsxbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BatchedCorpus():\n",
        "    def __init__(self, file_path:str):\n",
        "        self._file_path = file_path\n",
        "        \n",
        "    def get_batch(self, start=0, batch_size=100, preprocess=None):\n",
        "        \"\"\"\n",
        "            Args:\n",
        "                start (int) = row index where the batch starts\n",
        "                batch_size (int) = number of samples in the batch\n",
        "                preprocess (function) = function to preprocess text, i.e., remove special symbols etc.\n",
        "            Returns:\n",
        "                tuple (document class (int), document title (str), text (str))\n",
        "        \"\"\"\n",
        "        batch = []\n",
        "        labels = []\n",
        "        titles = []\n",
        "        texts = []\n",
        "        file = open(self._file_path, 'r') \n",
        "        for i, row in enumerate(itertools.islice(file, start, start + batch_size)):\n",
        "            line = file.readline().split('\",\"')\n",
        "            labels.append(int(re.sub(r'[^0-9]+', '', line[0])))\n",
        "            if preprocess:\n",
        "                texts.append(preprocess(line[2]))\n",
        "            else:\n",
        "                texts.append(line[2])\n",
        "        return labels, texts\n",
        "    \n",
        "    def iter_batch(self, batch_size=100, preprocess=None):\n",
        "      \"\"\" generator based on get_batch \"\"\"\n",
        "        start = 0\n",
        "        i = 0\n",
        "        labels, texts = self.get_batch(start, batch_size, preprocess)\n",
        "        while len(texts):            \n",
        "            yield labels, texts\n",
        "            labels, texts = self.get_batch(i * batch_size, batch_size, preprocess)\n",
        "            i = i + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83itLyu-s0Xb",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHBuTzKCs3FM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "e8d58ffc-6a2c-46a1-ae44-25ddc20414d8"
      },
      "source": [
        "BATCH_SIZE = 1000\n",
        "vectorizer = HashingVectorizer(decode_error='ignore', n_features=2 ** 18,\n",
        "                                    alternate_sign=False)   \n",
        "embedding = Embedding(1000, BATCH_SIZE, 2 ** 18)\n",
        "\n",
        "# prepare the test set, it's a small dataset so we just import it\n",
        "test = pd.read_csv(r'/content/drive/My Drive/test.csv'.replace(\"\\\\\", \"/\"), names=[\"label\", \"title\", \"text\"])\n",
        "test.text.apply(doc_cleaner)\n",
        "X_test = vectorizer.transform(test.text)\n",
        "\n",
        "train = BatchedCorpus(r'/content/drive/My Drive/train.csv'.replace(\"\\\\\", \"/\"))\n",
        "\n",
        "clf = SGDClassifier()\n",
        "# only one epoch\n",
        "for i, (labels, texts) in enumerate(train.iter_batch(preprocess=doc_cleaner, batch_size=BATCH_SIZE)):\n",
        "    # train the SVM\n",
        "    X = vectorizer.transform(texts)\n",
        "    clf.partial_fit(X, labels, np.unique(labels))\n",
        "    if i % 10 == 0:\n",
        "      print(\"batch = {}, SVM test accuracy = {}\".format(i, clf.score(X_test, test.label.values)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch = 0, SVM test accuracy = 0.6267105263157895\n",
            "batch = 10, SVM test accuracy = 0.8119736842105263\n",
            "batch = 20, SVM test accuracy = 0.8306578947368422\n",
            "batch = 30, SVM test accuracy = 0.848421052631579\n",
            "batch = 40, SVM test accuracy = 0.8581578947368421\n",
            "batch = 50, SVM test accuracy = 0.8589473684210527\n",
            "batch = 60, SVM test accuracy = 0.861578947368421\n",
            "batch = 70, SVM test accuracy = 0.8653947368421052\n",
            "batch = 80, SVM test accuracy = 0.8621052631578947\n",
            "batch = 90, SVM test accuracy = 0.8693421052631579\n",
            "batch = 100, SVM test accuracy = 0.8736842105263158\n",
            "batch = 110, SVM test accuracy = 0.8759210526315789\n",
            "batch = 120, SVM test accuracy = 0.8736842105263158\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}