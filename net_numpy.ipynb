{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(vec):\n",
    "    \"\"\" Sigmoid activation function\n",
    "\n",
    "    Args:\n",
    "        vec: input features vector\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-vec))\n",
    "\n",
    "\n",
    "def diff_sigmoid(vec):\n",
    "    \"\"\" derivative of the sigmoid \n",
    "    \n",
    "    Args:\n",
    "        vec: vector with respect to differentiate for\n",
    "    \"\"\"\n",
    "    return sigmoid(vec) * (1. - sigmoid(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    def __init__(self, n_features, alpha=0.01):\n",
    "        \"\"\" Initalize weights and biases of the neural net \"\"\"\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.W1 = np.random.rand(10, n_features) * 0.01\n",
    "        self.W2 = np.random.rand(5, 10) * 0.01\n",
    "        self.W3 = np.random.rand(1, 5) * 0.01\n",
    "        \n",
    "        self.b1 = np.ones((10, 1))\n",
    "        self.b2 = np.ones((5, 1))\n",
    "        self.b3 = np.ones((1, 1))\n",
    "\n",
    "        # \"memory\" of the network\n",
    "        # we keep intermediate results needed for the backpropagation\n",
    "\n",
    "        self.z1 = None\n",
    "        self.a1 = None\n",
    "        self.z2 = None\n",
    "        self.a2 = None\n",
    "        self.z3 = None\n",
    "        self.a3 = None        \n",
    "\n",
    "    @staticmethod\n",
    "    def cross_entropy_loss(y_hat, y):\n",
    "        \"\"\" calculates the loss function \n",
    "        \n",
    "            Args:\n",
    "                y_hat: calculated output array\n",
    "                y:     ground truth array      \n",
    "        \"\"\"\n",
    "        y_hat = np.squeeze(y_hat)\n",
    "        loss = y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)\n",
    "        return - loss\n",
    "\n",
    "    def forward_prop(self, x):\n",
    "        \"\"\" Calculates the output of the neural net\n",
    "\n",
    "        Args:\n",
    "            x: input features vector\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.z1 = np.dot(self.W1, x) + self.b1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "\n",
    "        self.z2 = np.dot(self.W2, self.a1) + self.b2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "\n",
    "        self.z3 = np.dot(self.W3, self.a2) + self.b3\n",
    "        self.a3 = sigmoid(self.z3)  # output of the network\n",
    "\n",
    "        return self.a3\n",
    "    \n",
    "    def backprop(self, y_hat, y):\n",
    "        \"\"\" backpropagation \"\"\"\n",
    "        self.dCdz3 = y_hat - y\n",
    "        self.dCdW3 = np.dot(self.dCdz3, self.a2.T)\n",
    "        self.dCdb3 = self.dCdz3\n",
    "\n",
    "        self.dCdz2 = np.dot(self.W3.T, self.dCdz3) * diff_sigmoid(self.z2)\n",
    "        self.dCdW2 = np.dot(self.dCdz2, self.a1.T)\n",
    "        self.dCdb2 = self.dCdz2\n",
    "\n",
    "        self.dCdz1 = np.dot(self.W2.T, self.dCdz2) * diff_sigmoid(self.z1)\n",
    "        self.dCdW1 = np.dot(self.dCdz1, self.x.T)\n",
    "        self.dCdb1 = self.dCdz1        \n",
    "\n",
    "        self.update()\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\" update the weights and biases \"\"\"\n",
    "        self.W1 = self.W1 - self.alpha * np.squeeze(self.dCdW1)\n",
    "        self.W2 = self.W2 - self.alpha * self.dCdW2\n",
    "        self.W3 = self.W3 - self.alpha * self.dCdW3\n",
    "\n",
    "        self.b1 = self.b1 - self.alpha * self.dCdb1\n",
    "        self.b2 = self.b2 - self.alpha * self.dCdb2\n",
    "        self.b3 = self.b3 - self.alpha * self.dCdb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, loss = 0.6347163398117263\n",
      "Epoch = 2, loss = 0.6195976882443531\n",
      "Epoch = 3, loss = 0.6147783553983158\n",
      "Epoch = 4, loss = 0.617645683945025\n",
      "Epoch = 5, loss = 0.5855294441090069\n",
      "Epoch = 6, loss = 0.4313916820115394\n",
      "Epoch = 7, loss = 0.2387235888549837\n",
      "Epoch = 8, loss = 0.14500421380456258\n",
      "Epoch = 9, loss = 0.09909237651155516\n",
      "Epoch = 10, loss = 0.07345501404407873\n",
      "Epoch = 11, loss = 0.057673546790961726\n",
      "Epoch = 12, loss = 0.04717562462828655\n",
      "Epoch = 13, loss = 0.0397631454605697\n",
      "Epoch = 14, loss = 0.03428298466146598\n",
      "Epoch = 15, loss = 0.03008276779232934\n",
      "Epoch = 16, loss = 0.026769549587868695\n",
      "Epoch = 17, loss = 0.024094091444682\n",
      "Epoch = 18, loss = 0.021891376376924655\n",
      "Epoch = 19, loss = 0.020048149144265927\n",
      "Epoch = 20, loss = 0.01848427523327634\n",
      "Epoch = 21, loss = 0.017141556479768498\n",
      "Epoch = 22, loss = 0.015976762215016885\n",
      "Epoch = 23, loss = 0.014957142147036591\n",
      "Epoch = 24, loss = 0.014057452964755298\n",
      "Epoch = 25, loss = 0.013257937470664\n",
      "Epoch = 26, loss = 0.012542919882668145\n",
      "Epoch = 27, loss = 0.01189980964653834\n",
      "Epoch = 28, loss = 0.01131838211796135\n",
      "Epoch = 29, loss = 0.010790250654229413\n",
      "Epoch = 30, loss = 0.01030847343118824\n",
      "Epoch = 31, loss = 0.009867256647655975\n",
      "Epoch = 32, loss = 0.009461727723604816\n",
      "Epoch = 33, loss = 0.009087760023902472\n",
      "Epoch = 34, loss = 0.008741835991030135\n",
      "Epoch = 35, loss = 0.008420939242330233\n",
      "Epoch = 36, loss = 0.008122468744641969\n",
      "Epoch = 37, loss = 0.007844169984705105\n",
      "Epoch = 38, loss = 0.00758407934474447\n",
      "Epoch = 39, loss = 0.00734047882681243\n",
      "Epoch = 40, loss = 0.007111858952872338\n",
      "Epoch = 41, loss = 0.006896888172774293\n",
      "Epoch = 42, loss = 0.006694387489294461\n",
      "Epoch = 43, loss = 0.006503309293361919\n",
      "Epoch = 44, loss = 0.0063227196182832025\n",
      "Epoch = 45, loss = 0.006151783186926621\n",
      "Epoch = 46, loss = 0.005989750753247117\n",
      "Epoch = 47, loss = 0.0058359483385534965\n",
      "Epoch = 48, loss = 0.005689768040377371\n",
      "Epoch = 49, loss = 0.00555066015281267\n",
      "Epoch = 50, loss = 0.005418126385506201\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'figsize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-415acbc020e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mepoch_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"iteration\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'figsize' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    iris = datasets.load_iris()\n",
    "    df = np.c_[iris.data, iris.target]\n",
    "\n",
    "    # take only two classes for a binary classification problem\n",
    "    rows = np.c_[np.where(df[:, -1]==1), np.where(df[:, -1]==0)][0]\n",
    "    df = df[rows]\n",
    "    np.random.shuffle(df[rows])\n",
    "\n",
    "    X = df[:, :-1]\n",
    "    y = df[:, -1]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "    the_net = network(alpha=0.2, n_features=X_train.shape[1])\n",
    "\n",
    "    epoch_loss = []\n",
    "    for epoch in range(0, 50):\n",
    "        # train and update one sample at a time\n",
    "        for sample in range(X_train.shape[0]):\n",
    "            x = np.expand_dims(X_train[sample, :], axis=1)\n",
    "            # forward pass\n",
    "            y_hat = the_net.forward_prop(x)\n",
    "            # compute the loss\n",
    "            loss = the_net.cross_entropy_loss(y_hat, y_train[sample])   \n",
    "            # backpropagate and update weights and biases\n",
    "            the_net.backprop(y_hat, y_train[sample])\n",
    "\n",
    "            # print(1)\n",
    "\n",
    "        print(\"Epoch = {}, loss = {}\".format(epoch + 1, loss))\n",
    "        epoch_loss.append(loss)\n",
    "\n",
    "    plt.figure(figsize(12, 8))\n",
    "    plt.plot(epoch_loss, marker=\".\")\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.show()\n",
    "    print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.2 64-bit",
   "language": "python",
   "name": "python36264bitec4441131f6b473bac97b47a7653e3c3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
