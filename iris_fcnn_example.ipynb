{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Iris dataset classification with Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-class classification of the iris dataset with a fully connected net. The network takes 4 input features and outputs the probability (softmax) of each sample in the training batch to belong to each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class fcnn(nn.Module):\n",
    "    def __init__(self, input_features=4, hidden_size=5, output_classes=3):\n",
    "        \"\"\" iris dataset has 4 features and 3 flower species (classes) \"\"\"\n",
    "        super(fcnn, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_features, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "df = np.c_[iris.data, iris.target]\n",
    "\n",
    "np.random.shuffle(df)\n",
    "\n",
    "X = df[:, :-1]\n",
    "y = df[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "y_test = torch.from_numpy(y_test).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make an instance of the network using the class *fcnn* above then we define the optimizer. We use the [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). The third fundamental ingredient is defining the loss function. We use [cross entropy](https://pytorch.org/docs/master/nn.html#torch.nn.CrossEntropyLoss). Different python packages have slightly different definitions of cross-entropy but Torch's documentation tells us that this function is suited for multi class classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the network\n",
    "net = fcnn()\n",
    "net = net.float()\n",
    "# print(net)\n",
    "\n",
    "# define the optimizer \n",
    "optimizer = optim.SGD(net.parameters(), lr=0.05)    \n",
    "# define the loss\n",
    "loss_fun = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we program a loop to train the network with batch training, that is we take a batch of 10 samples at a time and predict their classes and calculate the network parameter updates with *loss.backward()*. At this point we do not update the parameters of the network, but just calculate the gradients that are necessary for the update. We keep accumulating (adding up) the updates until the end of the outer loop, that is, until the end of the epoch and only then we perform the weights/biases update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss = 1.3795559406280518\n",
      "Epoch 100, loss = 0.5754812955856323\n",
      "Epoch 200, loss = 0.5558621287345886\n",
      "Epoch 300, loss = 0.5525184869766235\n",
      "Epoch 400, loss = 0.5520042181015015\n",
      "Epoch 500, loss = 0.5517762899398804\n",
      "Epoch 600, loss = 0.5516618490219116\n",
      "Epoch 700, loss = 0.5516020655632019\n",
      "Epoch 800, loss = 0.5515667200088501\n",
      "Epoch 900, loss = 0.5515437126159668\n",
      "test set accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "batch_size = 10\n",
    "epoch_loss = []\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "\n",
    "        x_b = X_train[i: i + batch_size].float()\n",
    "        y_b = y_train[i: i + batch_size]\n",
    "\n",
    "        y_hat = net(x_b)\n",
    "        loss = loss_fun(y_hat.squeeze(), y_b.squeeze()) \n",
    "        loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss.append(loss)\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch {}, loss = {}\".format(epoch, loss))\n",
    "\n",
    "# test accuracy\n",
    "predicted = net(X_test)\n",
    "_, y_pred = torch.max(predicted, 1)  # output 1 = max, output 2 = argmax\n",
    "\n",
    "print('test set accuracy', accuracy_score(y_test.data, y_pred.data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.2 64-bit",
   "language": "python",
   "name": "python36264bitec4441131f6b473bac97b47a7653e3c3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
