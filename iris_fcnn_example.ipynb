{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Iris dataset classification with Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notes provide a template to perform multi-class classification with PyTorch. In this example we want to use a 2 layers fully connected network. The loss function that we use, `CrossEntropyLoss()` expects an input of shape `(batch_size, C)` where C is the number of classes (3 in our case). Therefore our network outputs a vector of length 3 for each sample. Beware, as explained later, these numbers do not represent the probabilities of beloning to each class. Those probabilities are calculated for us inside the torch's function `CrossEntropyLoss()`. To get the probabilities of the input sample to belong to each class (a vector of length 3) we use the method `predict_prob()` in the class `fcnn`. It converts the raw network output in numbers that sum up to one, and can be interpreted as probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fcnn(nn.Module):\n",
    "    def __init__(self, input_features=4, hidden_size=5, output_classes=3):\n",
    "        \"\"\" iris dataset has 4 features and 3 flower species (classes) \"\"\"\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_features, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_classes)       \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "    def predict_prob(self, vec):\n",
    "        \"\"\" To predict probabilities (scores) we need to transform the output of the \n",
    "            network (1x3 array for each sample) to an 1x3 array of numbers that sum up\n",
    "            to 1 so that they can be interpreted as the probability of each class \n",
    "        \"\"\"\n",
    "        return self.softmax(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "df = np.c_[iris.data, iris.target]\n",
    "\n",
    "np.random.shuffle(df)\n",
    "\n",
    "X = df[:, :-1]\n",
    "y = df[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .5)\n",
    "\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "y_test = torch.from_numpy(y_test).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make an instance of the network using the class *fcnn* above then we define the optimizer. We use the [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). The third fundamental ingredient is defining the loss function. We use [cross entropy](https://pytorch.org/docs/master/nn.html#torch.nn.CrossEntropyLoss). Different python packages have slightly different definitions of cross-entropy but Torch's [documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss) tells us that:\n",
    "1. this function is suited for multi class classification problems.\n",
    "2. We don't have to use the `softmax` activation at the output because `CrossEntropyLoss()` already performs it. \n",
    "3. We don't need to one-hot encode the classes (target vector `y`) but the target vector should be consist of the class indices (in our case 0, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the network\n",
    "net = fcnn()\n",
    "net = net.float()\n",
    "# print(net)\n",
    "\n",
    "# define the optimizer \n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1)    \n",
    "# define the loss\n",
    "loss_fun = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we program a loop to train the network with batch training, that is we take a batch of 20 samples at a time and predict their classes and calculate the network parameter updates with `loss.backward()`. At this point we do not update the parameters of the network, but just calculate the gradients that are necessary for the update. We keep accumulating (adding up) the updates until the end of the outer loop, that is, until the end of the epoch and only then we perform the weights/biases update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average loss = 5.9978363424306735e-05\n",
      "Epoch 100, average loss = 6.051295713405125e-05\n",
      "Epoch 200, average loss = 6.112334813224152e-05\n",
      "Epoch 300, average loss = 6.179229967528954e-05\n",
      "Epoch 400, average loss = 6.249862053664401e-05\n",
      "Epoch 500, average loss = 6.323461275314912e-05\n",
      "Epoch 600, average loss = 6.402952567441389e-05\n",
      "Epoch 700, average loss = 6.493267574114725e-05\n",
      "Epoch 800, average loss = 6.595510785700753e-05\n",
      "Epoch 900, average loss = 6.7038883571513e-05\n",
      "test set accuracy = 0.960\n",
      "one-versus-rest area under the curve = 0.999\n",
      "one-versus-one area under the curve = 0.999\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "batch_size = 20\n",
    "epoch_loss = []\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()        \n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "\n",
    "        x_b = X_train[i: i + batch_size].float()\n",
    "        y_b = y_train[i: i + batch_size]\n",
    "\n",
    "        y_hat = net(x_b)\n",
    "        loss = loss_fun(y_hat, y_b) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss)\n",
    "    \n",
    "    epoch_loss.append(loss)\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch {}, average loss = {}\".format(epoch, loss/epochs))\n",
    "\n",
    "# test accuracy\n",
    "predicted = net(X_test)\n",
    "_, y_pred = torch.max(predicted, 1)  # predicted classes\n",
    "scores = net.predict_prob(predicted)  # predicted probabilities of each class for each sample\n",
    "\n",
    "print('test set accuracy = {:1.3f}'.format(accuracy_score(y_test, y_pred)))\n",
    "auc_ovr = roc_auc_score(y_true=y_test.detach().numpy(), y_score=scores.detach().numpy(), multi_class=\"ovr\")\n",
    "auc_ovo = roc_auc_score(y_true=y_test.detach().numpy(), y_score=scores.detach().numpy(), multi_class=\"ovo\")\n",
    "print('one-versus-rest area under the curve = {:1.3f}'.format(auc_ovr))\n",
    "print('one-versus-one area under the curve = {:1.3f}'.format(auc_ovo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24,  0,  0],\n",
       "       [ 0, 25,  1],\n",
       "       [ 0,  2, 23]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test.detach().numpy(), y_pred.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one sample that belongs to class 2 has been predicted as class 3, and two samples of class 3 have been predicted as class 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check manually some test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 1, 0, 1, 1, 0, 2, 0, 1, 1, 1, 0, 1, 0, 2, 0, 2, 2, 0, 0])\n",
      "tensor([2, 1, 0, 1, 1, 0, 2, 0, 2, 1, 1, 0, 1, 0, 2, 0, 2, 2, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[:20])\n",
    "print(y_test[:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.2 64-bit",
   "language": "python",
   "name": "python36264bitec4441131f6b473bac97b47a7653e3c3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
